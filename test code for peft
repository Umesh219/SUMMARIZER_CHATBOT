from transformers import AutoModelForSeq2SeqLM, AutoTokenizer

class DialogueSummarizer:
    def __init__(self, model_path):
        self.model = AutoModelForSeq2SeqLM.from_pretrained(model_path)
        self.tokenizer = AutoTokenizer.from_pretrained(model_path)

    def generate_summary(self, dialogues):
        dialogue_string = "\n".join(dialogues)
        prompt = f"Summarize the following conversation.\n\n{dialogue_string}\n\nSummary:"
        inputs = self.tokenizer(prompt, return_tensors='pt')
        output = self.model.generate(inputs["input_ids"], max_length=150, num_beams=4, early_stopping=True)
        summary = self.tokenizer.decode(output[0], skip_special_tokens=True)
        return summary

# Test the class with the provided test dialogues
test_dialogues = [
    "Agent: Hi there! How can I help you today?",
    "User: Hello, I placed an order yesterday for a new phone case. Can you give me an update on the delivery?",
    "Agent: Sure, let me check. Your order (ID: 12345) is currently being processed and is expected to be shipped within 2 business days.",
    "User: Great, thanks for letting me know! Do you have a tracking number for the shipment yet?",
    "Agent: Unfortunately, a tracking number won't be available until your order is shipped. You'll receive an email notification with the tracking information once it's on its way."
]

# Path to the fine-tuned PEFT model directory
peft_model_path = "C:/Users/saket.singh1/Downloads/FINE_TUNED_MODEL_WITH_PEFT"

# Instantiate the DialogueSummarizer class with the model path
summarizer = DialogueSummarizer(peft_model_path)

# Generate summary for the test dialogues
generated_summary = summarizer.generate_summary(test_dialogues)

print("Generated Summary:")
print(generated_summary)
